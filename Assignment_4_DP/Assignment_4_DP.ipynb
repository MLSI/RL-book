{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1: Manual Value Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Initialize Value Function: $v_0(s_1) = 10, v_0(s_2)=1, v_0(s_3)$.\n",
    "- Iteration 1: \n",
    "\n",
    "$$\\begin{aligned}\n",
    "q_1(s_1, a_1) &= 8 +0.2*10+0.6*1+0=10.6\\\\\n",
    "q_1(s_1, a_2) &= 10 +0.1*10+0.2*1+0=11.2\\\\\n",
    "v_1(s_1) &= max(10.6, 11.2) = 11.2\\\\\n",
    "\\pi_1(s_1) &= a_2\n",
    "\\end{aligned}$$\n",
    "\n",
    "$$\\begin{aligned}\n",
    "q_1(s_2, a_1) &= 1 +0.3*10+0.3*1+0=4.3\\\\\n",
    "q_1(s_2, a_2) &= -1 +0.5*10+0.3*1+0=4.3\\\\\n",
    "v_1(s_2) &= max(4.3, 4.3) = 4.3\\\\\n",
    "\\pi_1(s_2) &= a_1\n",
    "\\end{aligned}$$\n",
    "\n",
    "- Iteration 2:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "q_2(s_1, a_1) &= 8 +0.2*11.2+0.6*4.3+0=12.82\\\\\n",
    "q_2(s_1, a_2) &= 10 +0.1*11.2+0.2*4.3+0=11.98\\\\\n",
    "v_2(s_1) &= max(12.82, 11.98) = 12.82\\\\\n",
    "\\pi_2(s_1) &= a_1\n",
    "\\end{aligned}$$\n",
    "\n",
    "$$\\begin{aligned}\n",
    "q_2(s_2, a_1) &= 1 +0.3*11.2+0.3*4.3+0=5.65\\\\\n",
    "q_2(s_2, a_2) &= -1 +0.5*11.2+0.3*4.3+0=5.89\\\\\n",
    "v_2(s_2) &= max(5.65,5.89) =5.89\\\\\n",
    "\\pi_2(s_2) &= a_2\n",
    "\\end{aligned}$$\n",
    "\n",
    "- Why only needs k=2:\n",
    "$$\\begin{aligned}\n",
    "q_k(s_1, a_1) - q_k(s_1, a_2) &= 8-10 + (0.2-0.1)v_{k-1}(s_1)+ (0.6-0.2)v_{k-1}(s_2)\\\\\n",
    "&= -2 + 0.1v_{k-1}(s_1)+ 0.4 v_{k-1}(s_2) ~ \\text{for all } k\\geq 1\\\\\n",
    "q_k(s_2, a_1) - q_k(s_2, a_2) &= 1-(-1) + (0.3-0.5)v_{k-1}(s_1)+ (0.3-0.3)v_{k-1}(s_2)\\\\\n",
    "&= 2 -0.2v_{k-1}(s_1) ~ \\text{for all } k\\geq 1\n",
    "\\end{aligned}$$\n",
    "\n",
    "Since $v_k(s_1) \\geq 12.82$ and $v_k(s_2) \\geq 5.89$ for all $k \\geq 3$, we have\n",
    "$$\\begin{aligned}\n",
    "q_k(s_1, a_1) - q_k(s_1, a_2) \\geq -2 + 0.1*12.82+ 0.4 *5.89 = 1.638 >0 ~ \\text{for all } k\\geq 3\\\\\n",
    "q_k(s_2, a_1) - q_k(s_2, a_2) \\leq 2 -0.2*12.82 = -0.56 <0  ~ \\text{for all } k\\geq 3\n",
    "\\end{aligned}$$\n",
    "\n",
    "Hence $q_k(s_1, a_1) >q_k(s_1, a_2)$ and  $q_k(s_2, a_2) >q_k(s_2, a_1)$ for all $k\\geq 3$. Therefore we have $\\pi_k(s_1) = a_1$ and $\\pi_k(s_2) = a_2$ for all $k\\geq3$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2: Frog Croaking revisited\n",
    "\n",
    "(refer back to Q3 on Assignment 3)\n",
    "\n",
    "Task: \n",
    "- Compute Optimal Value Function and Optimal Policy Using Policy Iteration Algorithm & Value Iteration Algorithm;  - Analyze computational efficiency in terms of speed of convergence to the optimal value function. \n",
    "- How do those 2 iteration methods compare with brute force method of evaluating MDP for $2^{n-1}$ determimistic policy in previous assignment?\n",
    "- Plot graphs of convergence for different values of n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path\n",
    "sys.path.append('/Users/mulingsi/Desktop/MSE346/RL-book/')\n",
    "from dataclasses import dataclass\n",
    "from typing import Tuple, Dict\n",
    "from rl.markov_decision_process import FiniteMarkovDecisionProcess\n",
    "from rl.markov_decision_process import FinitePolicy, StateActionMapping\n",
    "from rl.markov_process import FiniteMarkovProcess, FiniteMarkovRewardProcess\n",
    "from rl.distribution import Categorical, Constant\n",
    "from scipy.stats import poisson\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class LilypadState:\n",
    "    lilypad: int\n",
    "        \n",
    "LilypadSoundMapping = StateActionMapping[LilypadState, str]\n",
    "\n",
    "class FrogEscapeMDP(FiniteMarkovDecisionProcess[LilypadState, str]):\n",
    "    \n",
    "    def __init__(self, num_lilypads:int):\n",
    "        self.num_lilypads = num_lilypads \n",
    "        super().__init__(self.get_action_transition_reward_map())\n",
    "\n",
    "    def get_action_transition_reward_map(self) -> LilypadSoundMapping:\n",
    "        d: Dict[LilypadState, Dict[str, Categorical[Tuple[LilypadState,float]]]] = {}\n",
    "            \n",
    "        d[0] = None\n",
    "        d[self.num_lilypads] = None\n",
    "        \n",
    "        for lilypad in range(1,self.num_lilypads):\n",
    "            state: LilypadState = LilypadState(lilypad)\n",
    "            d1: Dict[str,Categorical[Tuple[LilypadState, float]]] = {}\n",
    "\n",
    "            # sound A\n",
    "            sr_probs_dict_a: Dict[Tuple[LilypadState, float], float] = {}\n",
    "            sr_probs_dict_a[(LilypadState(lilypad - 1), 0)] = lilypad / (self.num_lilypads)\n",
    "            if lilypad + 1 == self.num_lilypads:\n",
    "                sr_probs_dict_a[(LilypadState(lilypad + 1), 1)] = 1 - lilypad/(self.num_lilypads)\n",
    "            else:\n",
    "                sr_probs_dict_a[(LilypadState(lilypad + 1), 0)] = 1 - lilypad /(self.num_lilypads)\n",
    "            d1['A'] = Categorical(sr_probs_dict_a)\n",
    "\n",
    "            # sound B\n",
    "            sr_probs_dict_b: Dict[Tuple[LilypadState, float], float] = {}\n",
    "            for i in range(self.num_lilypads+1):\n",
    "                if i != lilypad:\n",
    "                    if i == self.num_lilypads :\n",
    "                        sr_probs_dict_b[(LilypadState(i), 1)] = 1/(self.num_lilypads)\n",
    "                    else:\n",
    "                        sr_probs_dict_b[(LilypadState(i), 0)] = 1 /(self.num_lilypads)\n",
    "            d1['B'] = Categorical(sr_probs_dict_b)\n",
    "\n",
    "            d[state] = d1\n",
    "        return d\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl.dynamic_programming import value_iteration_result, policy_iteration_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_val(number_of_lilypads: int, user_gamma=int):\n",
    "    frog_mdp: FrogEscapeMDP = FrogEscapeMDP(number_of_lilypads)\n",
    "    # CREATING ALL POSSIBLE DETERMINISTIC POLICIES\n",
    "    policies = list(itertools.product(['A', 'B'], repeat=number_of_lilypads-1)) \n",
    "    val_function_map = {}\n",
    "    for policy in policies:\n",
    "        fdp: FinitePolicy[LilypadState, int] = FinitePolicy(\n",
    "            {LilypadState(padnum):\n",
    "            Constant(policy[padnum-1 ]) for padnum in range(1,number_of_lilypads)}\n",
    "        )\n",
    "        implied_mrp_tmp: FiniteMarkovRewardProcess[LilypadState] =\\\n",
    "            frog_mdp.apply_finite_policy(fdp)\n",
    "        val_function_map[policy] = implied_mrp_tmp.get_value_function_vec(gamma=user_gamma)\n",
    "    return val_function_map\n",
    "def find_optimal(val_function_map):\n",
    "    V_star = val_function_map[list(val_function_map.keys())[0]]\n",
    "    for i, policy in enumerate(val_function_map):\n",
    "        if np.all(val_function_map[policy] >= V_star) :\n",
    "            V_star = val_function_map[policy] \n",
    "    pol_star = []        \n",
    "    for policy in val_function_map.keys():\n",
    "        if np.all(val_function_map[policy]  == V_star):\n",
    "            pol_star = pol_star + [policy]\n",
    "    return V_star, pol_star"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_summary = pd.DataFrame(index=range(2, 15), columns =['value_iteration','policy_iteration','brute_force'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import itertools\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_summary = pd.DataFrame(index=range(2, 20), columns =['value_iteration','policy_iteration','brute_force'])\n",
    "for number_of_lilypads in range(2, 20):\n",
    "    frog_mdp: FrogEscapeMDP = FrogEscapeMDP(number_of_lilypads)\n",
    "    t1=time.time()\n",
    "    opt_vf_vi, opt_policy_vi = value_iteration_result(frog_mdp, gamma=1)\n",
    "    time_summary.loc[number_of_lilypads]['value_iteration'] = time.time()-t1\n",
    "    t1=time.time()\n",
    "    opt_vf_pi, opt_policy_pi = policy_iteration_result(frog_mdp, gamma=1)\n",
    "    time_summary.loc[number_of_lilypads]['policy_iteration'] = time.time()-t1\n",
    "    t1=time.time()\n",
    "    brute_vi, brute_pi = find_optimal(get_all_val(number_of_lilypads=number_of_lilypads, user_gamma=1))\n",
    "    time_summary.loc[number_of_lilypads]['brute_force'] = time.time()-t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_summary.plot(title='time till convergence by different methods')\n",
    "plt.xlabel('# of lilypads');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Value iteration is the fastest to converge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3: Job-Hopping and Wages-Utility-Maximization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup:\n",
    "\n",
    "- Start of the Day: {E, U}\n",
    "\n",
    "- During the day\n",
    "\n",
    "    - If employed, earn wage $w_n$; \n",
    "    \n",
    "    - If unemployed, randomly offered one of n jobs with respective probability. Choose accept/decline. If accept, earn $w_n$, else get unemployment wage $w_0$.\n",
    "    \n",
    "- End of day, lose job with prob $\\alpha \\in [0,1]$\n",
    "\n",
    "Objective: maximize infinite horizon expected discounted sum of wage utility. $max \\mathbb{E} [\\sum_{u=t}^\\infty \\gamma^{u-t} \\cdot log(w_i)]$\n",
    "\n",
    "$\\gamma \\in [0, 1$, $U(w) = log(w)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Notations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Solve in python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 4: 2-stores inventory control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl-book",
   "language": "python",
   "name": "rl-book"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
